#!/usr/bin/env python3
"""
Complete Evolution Test for CoT-Evo on ChemCoTDataset.

This script tests the COMPLETE evolutionary CoT distillation process from scratch:
1. Load queries from dataset (NOT using existing CoTs)
2. Use LLM teachers to generate INITIAL population (multi-thinker generation)
3. Evaluate fitness using multiple metrics
4. Apply novelty-driven selection (NSLC)
5. Perform reflective crossover and mutation (on reasoning part only)
6. Evolve trajectories over multiple generations
7. Output high-quality evolved CoTs

IMPORTANT: This script uses the Supplementary Material format (<|think|>/<|answer|>):
- Model outputs should be: "<|think|>\\n{reasoning}\\n<|answer|>\\n{answer}"
- We extract the part between <|think|> and <|answer|> as reasoning
- We extract the part after <|answer|> as the final answer
- During crossover/mutation, we only operate on the reasoning part
- The answer is preserved separately and re-attached at the end
"""

import asyncio
import json
import logging
import sys
import re
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional
from datetime import datetime

# Add src to path
sys.path.insert(0, str(Path(__file__).parent / "src"))

from src.models.registry import ModelRegistry
from src.initialization.generators import MultiThinkerGenerator
from src.selection.nslc import NSLCSelector
from src.variation.crossover import ReflectiveCrossover
from src.variation.mutation import ReflectiveMutation
from src.core.fitness import FitnessEvaluator
from src.core.trajectory import Trajectory
from src.knowledge.hybrid import HybridKnowledgeAugmenter
from src.knowledge.generation import KnowledgeGenerator
from src.utils.answer_extractor import (
    extract_cot_and_answer,
    extract_cot_from_markers,
    combine_cot_and_answer,
    clean_answer
)
from src.utils.evolution_logger import EvolutionLogger

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('test_complete_evolution.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


# ============================================================================
# Data Loading
# ============================================================================

def load_chemcot_dataset(data_path: str, max_samples: int = 10) -> List[Dict[str, Any]]:
    """
    Load ChemCoT dataset from JSON file.

    Args:
        data_path: Path to the JSON file
        max_samples: Maximum number of samples to load

    Returns:
        List of dataset samples
    """
    logger.info(f"Loading ChemCoT dataset from {data_path}")

    if not Path(data_path).exists():
        raise FileNotFoundError(f"Dataset not found: {data_path}")

    with open(data_path, 'r') as f:
        data = json.load(f)

    # Load first max_samples samples
    samples = data[:max_samples]

    logger.info(f"Loaded {len(samples)} samples")
    return samples


def prepare_sample_for_evolution(sample: Dict[str, Any]) -> Dict[str, Any]:
    """
    Prepare a dataset sample for evolution.

    IMPORTANT: We only extract query and ground truth, NOT existing CoTs.
    The initial population will be generated by LLM teachers.

    Args:
        sample: A dataset sample

    Returns:
        Prepared sample with query and ground_truth
    """
    query = sample['query']

    # Extract ground truth from meta
    meta = sample.get('meta', {})
    if isinstance(meta, dict):
        ground_truth = meta.get('gt', '')
    else:
        ground_truth = str(meta) if meta else ''

    return {
        'id': sample['id'],
        'query': query,
        'ground_truth': ground_truth,
        'task': sample.get('task', ''),
    }


def calculate_reasoning_length(reasoning: str) -> int:
    """
    Calculate reasoning length, excluding <|think|> and <|answer|> markers.

    This ensures consistency with EvolutionLogger's reasoning_length calculation.

    Args:
        reasoning: Full reasoning string potentially containing markers

    Returns:
        Length of pure reasoning content (character count)
    """
    if not reasoning:
        return 0

    # Extract content between <|think|> and <|answer|>
    if "<|think|>" in reasoning:
        start = reasoning.find("<|think|>") + 9
        end = reasoning.find("<|answer|>")
        if end != -1:
            reasoning = reasoning[start:end].strip()

    return len(reasoning)


# ============================================================================
# Enhanced Multi-Thinker Generator with Answer Extraction
# ============================================================================

class EnhancedMultiThinkerGenerator:
    """
    Enhanced multi-thinker generator that properly separates reasoning and answer.
    """

    def __init__(
        self,
        model_registry: ModelRegistry,
        knowledge_augmenter: HybridKnowledgeAugmenter,
    ):
        """
        Initialize the enhanced generator.

        Args:
            model_registry: Registry of available teacher models
            knowledge_augmenter: Knowledge augmenter for Formula 2
        """
        self.registry = model_registry
        self.knowledge_augmenter = knowledge_augmenter

    async def generate_initial_pool(
        self,
        query: str,
        ground_truth: str,
        n_vanilla: int = 4,
        n_knowledge_augmented: int = 2,
    ) -> List[Trajectory]:
        """
        Generate initial candidate pool P = P^G âˆª P^K.

        Args:
            query: The input problem/question
            ground_truth: The correct answer (used for knowledge generation)
            n_vanilla: Number of vanilla-generated trajectories
            n_knowledge_augmented: Number of knowledge-augmented trajectories

        Returns:
            List of generated trajectories
        """
        logger.info(
            f"Generating initial pool: {n_vanilla} vanilla + "
            f"{n_knowledge_augmented} knowledge-augmented"
        )

        tasks = []

        # Generate vanilla trajectories
        for i in range(n_vanilla):
            model = self.registry.get_random_thinker()
            if model is None:
                logger.warning("No teacher models available for vanilla generation")
                continue
            tasks.append(self._generate_vanilla(model, query))

        # Generate knowledge-augmented trajectories
        if n_knowledge_augmented > 0:
            try:
                knowledge = await self.knowledge_augmenter.generate_knowledge(query, ground_truth)
                logger.info(f"Generated knowledge ({len(knowledge)} chars)")

                for i in range(n_knowledge_augmented):
                    model = self.registry.get_random_thinker()
                    if model is None:
                        logger.warning("No teacher models available for knowledge generation")
                        continue
                    tasks.append(
                        self._generate_knowledge_augmented(model, query, knowledge)
                    )

            except Exception as e:
                logger.error(f"Failed to generate knowledge: {e}. Skipping knowledge-augmented trajectories.")

        # Execute all generation tasks in parallel
        if not tasks:
            logger.error("No trajectories generated!")
            return []

        trajectories = await asyncio.gather(*tasks, return_exceptions=True)

        # Filter out failed generations
        valid_trajectories = []
        for i, traj in enumerate(trajectories):
            if isinstance(traj, Exception):
                logger.warning(f"Trajectory {i} generation failed: {traj}")
            elif traj is not None:
                valid_trajectories.append(traj)

        logger.info(f"Successfully generated {len(valid_trajectories)} trajectories")
        return valid_trajectories

    async def _generate_vanilla(self, model, query: str) -> Trajectory:
        """Generate a vanilla CoT."""
        prompt = f"""Please solve the following chemistry problem step by step.

Your response must follow this exact format:
<|think|>
Your step-by-step reasoning process goes here. Explain your thought process clearly.
<|answer|>
Your final answer in JSON format: {{"Major Product": "SMILES string"}}

Question: {query}

Remember to:
1. Think step by step
2. Show your work clearly
3. Provide your final answer in the specified JSON format"""

        messages = [
            {"role": "system", "content": "You are an expert chemist. Solve chemistry problems with careful reasoning."},
            {"role": "user", "content": prompt}
        ]

        # Generate full output
        full_output = await model.generate_async(prompt="", messages=messages)

        # Extract reasoning and answer using new format
        reasoning, answer = extract_cot_and_answer(full_output)

        return Trajectory(
            query=query,
            answer=answer,
            reasoning=reasoning,
            source_model=model.model_name,
            generation_method="vanilla"
        )

    async def _generate_knowledge_augmented(
        self,
        model,
        query: str,
        knowledge: str
    ) -> Trajectory:
        """Generate a knowledge-augmented CoT."""
        prompt = f"""Please solve the following chemistry problem step by step. Use the provided knowledge to help you.

Relevant knowledge:
{knowledge}

Your response must follow this exact format:
<|think|>
Your step-by-step reasoning process goes here. Make sure to use the provided knowledge effectively.
<|answer|>
Your final answer in JSON format: {{"Major Product": "SMILES string"}}

Question: {query}

Remember to:
1. Use the provided knowledge effectively
2. Think step by step
3. Show your work clearly
4. Provide your final answer in the specified JSON format"""

        messages = [
            {"role": "system", "content": "You are an expert chemist. Solve chemistry problems with careful reasoning."},
            {"role": "user", "content": prompt}
        ]

        # Generate full output
        full_output = await model.generate_async(prompt="", messages=messages)

        # Extract reasoning and answer using new format
        reasoning, answer = extract_cot_and_answer(full_output)

        return Trajectory(
            query=query,
            answer=answer,
            reasoning=reasoning,
            knowledge=knowledge,
            source_model=model.model_name,
            generation_method="knowledge_augmented"
        )


# ============================================================================
# Fitness Evaluation
# ============================================================================

class ChemCoTFitnessEvaluator:
    """
    Fitness evaluator for ChemCoT tasks aligned with Paper Formula 6.

    Implements: R(t) = s_EM + Î»1 * s_LEN + Î»2 * s_KNOW

    Where:
    - s_EM âˆˆ {0, 1}: Exact match (binary, no partial match)
    - s_LEN âˆˆ {0.0, 0.5, 1.0}: Length appropriateness (percentile-based)
    - s_KNOW âˆˆ {0, 1, 2, 3, 4, 5}: Knowledge usage correctness (LLM judge, normalized to [0,1])
    - Î»1, Î»2 are configurable weights
    """

    def __init__(
        self,
        ground_truth: str,
        lambda_length: float = 0.3,
        lambda_knowledge: float = 0.5,
        use_llm_judge: bool = False,
        judge_model=None,
        lower_percentile: int = 50,
        upper_percentile: int = 300
    ):
        """
        Initialize with ground truth answer and configuration.

        Args:
            ground_truth: The correct answer (SMILES string)
            lambda_length: Weight for length score (Î»1), default 0.3
            lambda_knowledge: Weight for knowledge score (Î»2), default 0.5
            use_llm_judge: Whether to use LLM judge for knowledge (default: False)
            judge_model: LLM model for judging (required if use_llm_judge=True)
            lower_percentile: Lower bound for reasoning length (words), default 50
            upper_percentile: Upper bound for reasoning length (words), default 300
        """
        self.ground_truth = ground_truth.strip().lower()
        self.lambda_length = lambda_length
        self.lambda_knowledge = lambda_knowledge
        self.use_llm_judge = use_llm_judge
        self.judge_model = judge_model
        self.lower_percentile = lower_percentile
        self.upper_percentile = upper_percentile

        # P2: Validate LLM judge configuration
        if self.use_llm_judge and self.judge_model is None:
            raise ValueError("judge_model is required when use_llm_judge=True")

    def evaluate(self, trajectory: Trajectory) -> float:
        """
        Compute fitness score using Paper Formula 6:
        R(t) = s_EM + Î»1 * s_LEN + Î»2 * s_KNOW

        Args:
            trajectory: The trajectory to evaluate

        Returns:
            Fitness score (higher is better)
        """
        # 1. Exact match (s_EM âˆˆ {0, 1})
        # P2: Remove partial match, keep only binary exact match
        answer = clean_answer(trajectory.answer).strip().lower()
        s_em = 1.0 if answer == self.ground_truth else 0.0

        # 2. Length appropriateness (s_LEN âˆˆ {0.0, 0.5, 1.0})
        # P2: Use percentile-based scoring instead of fixed thresholds
        reasoning_word_count = len(trajectory.reasoning.split())

        if reasoning_word_count < self.lower_percentile:
            s_len = 0.0  # Too short
        elif reasoning_word_count > self.upper_percentile:
            s_len = 0.5  # Too long
        else:
            s_len = 1.0  # Just right

        # 3. Knowledge usage correctness (s_KNOW âˆˆ {1, 2, 3, 4, 5})
        # P2: Use LLM judge when enabled, otherwise binary check
        if trajectory.knowledge and len(trajectory.knowledge) > 0:
            if self.use_llm_judge:
                # Use LLM-as-a-Judge to evaluate knowledge usage (1-5 scale)
                s_know_raw = self._evaluate_knowledge_with_llm(
                    trajectory.reasoning,
                    trajectory.knowledge
                )
                # Normalize to [0, 1] for formula application
                s_know_normalized = s_know_raw / 5.0
            else:
                # P2: Fallback to binary check when LLM judge disabled
                # Simply check if reasoning is non-empty and has knowledge
                if trajectory.reasoning and len(trajectory.reasoning.strip()) > 0:
                    s_know_normalized = 1.0  # Assume good usage
                else:
                    s_know_normalized = 0.0  # No reasoning, can't use knowledge
        else:
            s_know_normalized = 0.0  # No knowledge

        # Apply Paper Formula 6: R(t) = s_EM + Î»1 * s_LEN + Î»2 * s_KNOW
        fitness = s_em + (self.lambda_length * s_len) + (self.lambda_knowledge * s_know_normalized)

        logger.debug(
            f"Fitness breakdown: s_EM={s_em:.1f}, s_LEN={s_len:.1f}, "
            f"s_KNOW={s_know_normalized:.2f}, Î»1={self.lambda_length}, "
            f"Î»2={self.lambda_knowledge}, total={fitness:.3f}"
        )

        return fitness

    def _evaluate_knowledge_with_llm(self, reasoning: str, knowledge: str) -> int:
        """
        Evaluate knowledge usage correctness using LLM-as-a-Judge.

        Args:
            reasoning: The reasoning process
            knowledge: The reference knowledge

        Returns:
            Score from 1 to 5
        """
        # P2: Construct LLM judge prompt
        prompt = f"""You are evaluating how well a reasoning trajectory uses given knowledge.

Reference Knowledge:
{knowledge}

Reasoning to Evaluate:
{reasoning}

Your task: Assess how accurately and appropriately the reasoning uses the reference knowledge.
Consider:
1. Does the reasoning correctly interpret the knowledge?
2. Does the reasoning apply the knowledge appropriately?
3. Is the knowledge used to reach the correct conclusion?

Provide a score from 1 to 5:
- 1: Knowledge is misinterpreted or misused
- 2: Knowledge is used but with significant errors
- 3: Knowledge is used adequately but with minor issues
- 4: Knowledge is used well and correctly
- 5: Knowledge is used excellently with deep understanding

Output only the score as a single digit (1-5)."""

        try:
            # P2: Call LLM judge
            response = self.judge_model.generate(prompt)
            score = self._extract_score(response)
            logger.debug(f"LLM judge score: {score}/5")
            return score
        except Exception as e:
            logger.error(f"LLM judge evaluation failed: {e}")
            # Fallback to middle score on error
            return 3

    def _extract_score(self, response: str) -> int:
        """Extract the score from the judge's response."""
        import re

        # Look for the first number in the response
        match = re.search(r'\d+', response.strip())

        if match:
            score = int(match.group())
            # Clamp to valid range [1, 5]
            return max(1, min(5, score))
        else:
            # Default to middle score if parsing fails
            logger.warning(f"Could not extract score from LLM response: {response[:100]}")
            return 3


# ============================================================================
# Simplified Evolution Process
# ============================================================================

async def run_evolution(
    initial_trajectories: List[Trajectory],
    query: str,
    ground_truth: str,
    registry: ModelRegistry,
    selector: NSLCSelector,
    crossover: ReflectiveCrossover,
    mutation: ReflectiveMutation,
    sample_id: str,
    max_generations: int = 3,
    population_size: int = 6
) -> Trajectory:
    """
    Run simplified evolution process with detailed logging.

    This demonstrates the key concepts:
    1. Start with initial population
    2. Evaluate fitness
    3. Select parents using NSLC
    4. Create offspring through crossover and mutation
    5. Select best for next generation

    Args:
        initial_trajectories: Initial population from LLM teachers
        query: The query being solved
        ground_truth: Ground truth answer
        registry: Model registry
        selector: NSLC selector
        crossover: Crossover operator
        mutation: Mutation operator
        sample_id: Sample ID for logging
        max_generations: Number of generations
        population_size: Population size

    Returns:
        Best trajectory found
    """
    # Initialize evolution logger
    output_dir = Path("./outputs/test_complete_evolution")
    evo_logger = EvolutionLogger(output_dir, sample_id[:8])

    fitness_evaluator = ChemCoTFitnessEvaluator(ground_truth)
    population = initial_trajectories[:population_size]

    # Log initial population
    evo_logger.log_initial_population(population, gen=0)

    for generation in range(max_generations):
        logger.info(f"\n{'='*60}")
        logger.info(f"Generation {generation + 1}/{max_generations}")
        logger.info(f"{'='*60}")

        # Evaluate fitness for all trajectories
        for traj in population:
            traj.set_fitness_score(fitness_evaluator.evaluate(traj))

        # Log evaluation
        evo_logger.log_evaluation(population, generation + 1)

        # Sort by fitness
        population.sort(key=lambda t: t.fitness_score, reverse=True)

        # Log statistics
        fitnesses = [t.fitness_score for t in population]
        logger.info(f"Population size: {len(population)}")
        logger.info(f"Avg fitness: {sum(fitnesses)/len(fitnesses):.3f}")
        logger.info(f"Best fitness: {fitnesses[0]:.3f}")
        logger.info(f"Worst fitness: {fitnesses[-1]:.3f}")

        # Log best trajectory info
        best = population[0]
        logger.info(f"Best answer: {clean_answer(best.answer)}")
        logger.info(f"Best source: {best.source_model} ({best.generation_method})")

        # Early stopping: if we achieved perfect fitness, stop evolution
        if best.fitness_score is not None and best.fitness_score >= 1.0:
            logger.info(f"\n{'='*60}")
            logger.info(f"ðŸŽ¯ Early stopping: Perfect fitness {best.fitness_score:.3f} achieved!")
            logger.info(f"Stopped after generation {generation + 1}/{max_generations}")
            logger.info(f"{'='*60}\n")

            # Log early stop event
            evo_logger.log_early_stop(best, generation + 1, best.fitness_score)

            # Log final best
            evo_logger.log_final_best(best, generation + 1)

            return best

        # Selection: keep top performers (elitism)
        n_keep = max(2, int(population_size * 0.3))
        parents = population[:n_keep]
        logger.info(f"Keeping {n_keep} elites for next generation")

        # Log selection
        evo_logger.log_selection(parents, generation + 1)

        # Create offspring through mutation
        # We use innovative mutation to try to improve incorrect answers
        offspring = []
        mutation_ops = []

        for parent in parents[:n_keep//2]:
            # Only mutate if answer is incorrect
            parent_answer = clean_answer(parent.answer).strip().lower()
            is_correct = parent_answer == ground_truth.lower()

            if not is_correct:
                try:
                    # Use innovative mutation to improve the answer
                    mutated = await mutation.mutate(
                        trajectory=parent,
                        query=query,
                        ground_truth=ground_truth,
                        mode="innovate"
                    )

                    if mutated:
                        offspring.append(mutated)
                        mutation_ops.append({
                            "parent_id": parent.id,
                            "mode": "innovate",
                            "success": True,
                            "new_trajectory": evo_logger._traj_to_dict(mutated, len(offspring)),
                            "parent_fitness": parent.fitness_score,
                            "new_fitness": mutated.fitness_score if mutated.fitness_score else None
                        })
                        logger.debug(f"  Created offspring via innovative mutation")
                    else:
                        mutation_ops.append({
                            "parent_id": parent.id,
                            "mode": "innovate",
                            "success": False,
                            "error": "mutation returned None"
                        })
                except Exception as e:
                    logger.debug(f"  Mutation failed: {e}")
                    mutation_ops.append({
                        "parent_id": parent.id,
                        "mode": "innovate",
                        "success": False,
                        "error": str(e)
                    })
                    evo_logger.log_error("mutation_failed", str(e), {"parent_id": parent.id, "mode": "innovate"})
                    continue
            else:
                logger.debug(f"  Parent already correct, skipping mutation")

        # If we don't have enough offspring, add some random mutations
        while len(offspring) < population_size - n_keep:
            parent = parents[len(offspring) % len(parents)]
            try:
                # Use additive mutation to add more detail
                mutated = await mutation.mutate(
                    trajectory=parent,
                    query=query,
                    ground_truth=ground_truth,
                    mode="add"
                )
                if mutated:
                    offspring.append(mutated)
                    mutation_ops.append({
                        "parent_id": parent.id,
                        "mode": "add",
                        "success": True,
                        "new_trajectory": evo_logger._traj_to_dict(mutated, len(offspring))
                    })
                else:
                    # Fallback: copy parent
                    copy = Trajectory(
                        query=parent.query,
                        answer=parent.answer,
                        reasoning=parent.reasoning,
                        source_model=parent.source_model,
                        generation_method="selection"
                    )
                    offspring.append(copy)
                    mutation_ops.append({
                        "parent_id": parent.id,
                        "mode": "add",
                        "success": False,
                        "fallback": "copy"
                    })
            except Exception as e:
                logger.debug(f"  Fallback mutation failed: {e}")
                # Copy parent as last resort
                copy = Trajectory(
                    query=parent.query,
                    answer=parent.answer,
                    reasoning=parent.reasoning,
                    source_model=parent.source_model,
                    generation_method="selection"
                )
                offspring.append(copy)
                mutation_ops.append({
                    "parent_id": parent.id,
                    "mode": "add",
                    "success": False,
                    "error": str(e),
                    "fallback": "copy"
                })

        # Log mutation operations
        evo_logger.log_mutation(mutation_ops, generation + 1)

        # Form new population
        population = parents + offspring[:population_size - n_keep]

        # P1 FIX: Evaluate offspring before logging to avoid fitness=None in logs
        # New offspring from mutation/crossover need fitness evaluation
        new_offspring = offspring[:population_size - n_keep]
        for traj in new_offspring:
            traj.set_fitness_score(fitness_evaluator.evaluate(traj))

        logger.info(f"New population size: {len(population)}")

        # Log new generation (all trajectories now have fitness values)
        evo_logger.log_new_generation(population, generation + 1)

    # Return best trajectory (filter out trajectories without fitness score)
    valid_population = [t for t in population if t.fitness_score is not None]
    if not valid_population:
        logger.warning("No valid trajectories with fitness scores found")
        valid_population = population  # Fallback to all trajectories

    valid_population.sort(key=lambda t: t.fitness_score, reverse=True)
    best = valid_population[0]

    # Log final best
    evo_logger.log_final_best(best, max_generations)

    logger.info(f"\nFinal best fitness: {best.fitness_score:.3f}")
    return best



# ============================================================================
# Main Test Function
# ============================================================================

async def test_complete_evolution():
    """
    Test the complete CoT-Evo pipeline on ChemCoT dataset.

    This demonstrates the full evolutionary process from scratch:
    1. Load queries from dataset
    2. Use LLM teachers to generate initial population
    3. Run evolutionary algorithm
    4. Output evolved CoTs
    """
    logger.info("=" * 80)
    logger.info("CoT-Evo Complete Evolution Test")
    logger.info("Testing full pipeline from query to evolved CoT")
    logger.info("=" * 80)

    # ========================================================================
    # 1. Load Dataset
    # ========================================================================
    logger.info("\n[Step 1] Loading dataset...")
    data_path = "./data/ChemCoTDataset/train-GA-small.json"

    if not Path(data_path).exists():
        logger.error(f"Dataset not found: {data_path}")
        logger.info("Please ensure the dataset is available at this path.")
        return

    samples = load_chemcot_dataset(data_path, max_samples=10)

    # Prepare samples (only query + ground truth, no existing CoTs)
    prepared_samples = [prepare_sample_for_evolution(s) for s in samples]
    logger.info(f"Prepared {len(prepared_samples)} samples for evolution")

    # ========================================================================
    # 2. Initialize Model Registry
    # ========================================================================
    logger.info("\n[Step 2] Initializing model registry...")
    registry = ModelRegistry(config_path="config/models.yaml")
    logger.info(f"Loaded {len(registry.thinkers)} teacher models:")
    for thinker in registry.thinkers:
        logger.info(f"  - {thinker}")

    # ========================================================================
    # 3. Initialize Knowledge Generator
    # ========================================================================
    logger.info("\n[Step 3] Setting up knowledge augmentation...")
    # Get the knowledge generator model from registry
    knowledge_gen_model = registry.knowledge_generator_model
    knowledge_generator = KnowledgeGenerator(model=knowledge_gen_model)
    knowledge_augmenter = HybridKnowledgeAugmenter(
        generator=knowledge_generator,
        knowledge_base=None  # No RAG for this test
    )

    # ========================================================================
    # 4. Setup Enhanced Multi-Thinker Generator
    # ========================================================================
    logger.info("\n[Step 4] Setting up multi-thinker generator...")
    generator = EnhancedMultiThinkerGenerator(
        model_registry=registry,
        knowledge_augmenter=knowledge_augmenter
    )

    # ========================================================================
    # 5. Setup Evolution Components
    # ========================================================================
    logger.info("\n[Step 5] Setting up evolution components...")

    selector = NSLCSelector(
        model_registry=registry,
        n_neighbors=3,
        epsilon=0.1
    )

    crossover = ReflectiveCrossover(model_registry=registry)
    mutation = ReflectiveMutation(model_registry=registry)

    logger.info("Evolution components initialized")

    # ========================================================================
    # 6. Run Evolution on Each Sample (IN PARALLEL)
    # ========================================================================
    logger.info("\n[Step 6] Running evolution on samples in PARALLEL...")

    output_dir = Path("./outputs/test_complete_evolution")
    output_dir.mkdir(parents=True, exist_ok=True)

    # Define async function to process a single sample
    async def process_single_sample(idx, sample):
        """Process a single sample through the evolution pipeline."""
        logger.info(f"[Sample {idx + 1}/{len(prepared_samples)}] Starting: {sample['id'][:8]}")

        try:
            # Generate initial population using LLM teachers
            initial_trajectories = await generator.generate_initial_pool(
                query=sample['query'],
                ground_truth=sample['ground_truth'],
                n_vanilla=4,
                n_knowledge_augmented=2
            )

            if len(initial_trajectories) < 2:
                logger.warning(f"[Sample {idx + 1}] Only {len(initial_trajectories)} trajectories, skipping")
                return None

            logger.info(f"[Sample {idx + 1}] Generated {len(initial_trajectories)} initial trajectories")

            # Run evolution
            best_trajectory = await run_evolution(
                initial_trajectories=initial_trajectories,
                query=sample['query'],
                ground_truth=sample['ground_truth'],
                registry=registry,
                selector=selector,
                crossover=crossover,
                mutation=mutation,
                sample_id=sample['id'],
                max_generations=3,
                population_size=6
            )

            # Evaluate final fitness
            fitness_evaluator = ChemCoTFitnessEvaluator(sample['ground_truth'])
            final_fitness = fitness_evaluator.evaluate(best_trajectory)

            result = {
                'sample_id': sample['id'],
                'task': sample['task'],
                'query': sample['query'][:200] + "..." if len(sample['query']) > 200 else sample['query'],
                'ground_truth': sample['ground_truth'],
                'best_answer': clean_answer(best_trajectory.answer),
                'best_fitness': final_fitness,
                'best_reasoning_length': calculate_reasoning_length(best_trajectory.reasoning),
                'source_model': best_trajectory.source_model,
                'generation_method': best_trajectory.generation_method,
                'full_reasoning': best_trajectory.reasoning,
                'full_answer': best_trajectory.answer,
            }

            # Save individual result
            result_file = output_dir / f"sample_{idx}_{sample['id'][:8]}.json"
            with open(result_file, 'w') as f:
                detailed_result = {
                    **result,
                }
                json.dump(detailed_result, f, indent=2)

            logger.info(f"[Sample {idx + 1}] Completed: fitness={final_fitness:.3f}")

            return result

        except Exception as e:
            logger.error(f"[Sample {idx + 1}] Failed: {e}")
            return None

    # Process all samples in parallel
    logger.info(f"Launching {len(prepared_samples)} samples in parallel...")
    tasks = [
        process_single_sample(idx, sample)
        for idx, sample in enumerate(prepared_samples)
    ]

    # Wait for all tasks to complete
    sample_results = await asyncio.gather(*tasks, return_exceptions=True)

    # Filter out None results and exceptions
    results = []
    for i, result in enumerate(sample_results):
        if isinstance(result, Exception):
            logger.error(f"Sample {i} raised exception: {result}")
        elif result is not None:
            results.append(result)

    logger.info(f"\nSuccessfully completed {len(results)}/{len(prepared_samples)} samples")

    # ========================================================================
    # 7. Summary Statistics
    # ========================================================================
    logger.info("\n" + "=" * 80)
    logger.info("EVOLUTION SUMMARY")
    logger.info("=" * 80)

    if results:
        avg_fitness = sum(r['best_fitness'] for r in results) / len(results)
        max_fitness = max(r['best_fitness'] for r in results)

        logger.info(f"Total samples processed: {len(results)}")
        logger.info(f"Average fitness: {avg_fitness:.3f}")
        logger.info(f"Max fitness: {max_fitness:.3f}")

        # Count exact matches
        exact_matches = sum(
            1 for r in results
            if r['best_answer'].lower() == r['ground_truth'].lower()
        )
        logger.info(f"Exact matches: {exact_matches}/{len(results)} ({100*exact_matches/len(results):.1f}%)")

        # Save summary
        summary_file = output_dir / "summary.json"
        with open(summary_file, 'w') as f:
            json.dump({
                'timestamp': datetime.now().isoformat(),
                'total_samples': len(results),
                'avg_fitness': avg_fitness,
                'max_fitness': max_fitness,
                'exact_matches': exact_matches,
                'exact_match_rate': 100*exact_matches/len(results) if results else 0,
                'results': results
            }, f, indent=2)

    logger.info("\nTest completed successfully!")
    logger.info(f"Results saved to {output_dir}")


# ============================================================================
# Entry Point
# ============================================================================

if __name__ == "__main__":
    try:
        asyncio.run(test_complete_evolution())
    except KeyboardInterrupt:
        logger.info("\nTest interrupted by user")
    except Exception as e:
        logger.error(f"\nTest failed with error: {e}", exc_info=True)
        sys.exit(1)
